import streamlit as st
import os
from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import AzureSearch
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
#from langchain.callbacks import StreamlitCallbackHandler
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain.schema import Document
import json
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
from azure.search.documents.indexes import SearchIndexClient
from datetime import datetime
from azure.storage.blob import BlobServiceClient

#####

load_dotenv()

llm_api_key = os.getenv("LLM_API_KEY")    
llm_endpoint = os.getenv("LLM_ENDPOINT")  
llm_api_version = os.getenv("LLM_API_VERSION")  
llm_deployment_name = os.getenv("LLM_DEPLOYMENT_NAME")

llm_deployment = os.getenv("LLM_DEPLOYMENT_NAME")

embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")
embedding_endpoint = os.getenv("EMBEDDING_ENDPOINT")
embedding_api_key = os.getenv("EMBEDDING_API_KEY")
search_endpoint = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
search_key = os.getenv("AZURE_AI_SERACH_KEY")


st.markdown("""
<style>
    [data-testid="stSidebarNav"] {
        display: none !important;
    }
    .css-1d391kg {
        display: none !important;
    }
    .css-1rs6os {
        display: none !important;
    }
    .css-17ziqus {
        display: none !important;
    }
</style>
""", unsafe_allow_html=True)

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG ì‹œìŠ¤í…œ - Azure AI Search",
    page_icon="ğŸ”",
    layout="wide"
)


# ì‚¬ì´ë“œë°” ì„¤ì •
page = st.sidebar.selectbox(
    "í˜ì´ì§€ ì„ íƒ",
    ["ë©”ì¸ í˜ì´ì§€", "Page 1: ì§€ì‹ì •ë³´ ìƒì„±", "Page 2: ì§€ì‹ì •ë³´ ì €ì¥", "Page 3: ì§ˆë¬¸ ë° ê²€ìƒ‰"],index=3
    )
st.sidebar.markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
# st.sidebar.info("âœ… ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ ì¤‘")
st.sidebar.markdown(f"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# ë©”ì¸ í˜ì´ì§€
if page == "ë©”ì¸ í˜ì´ì§€":
    # í—¤ë”
    st.title("ğŸ§  ì§€ì‹ ì •ë³´ ê´€ë¦¬ ì‹œìŠ¤í…œ")
    # st.markdown("### íš¨ìœ¨ì ì¸ ì§€ì‹ ì •ë³´ ìƒì„±, ì„ë² ë”©, ê²€ìƒ‰ì„ ìœ„í•œ í†µí•© í”Œë«í¼")
    st.switch_page("./main.py")


elif page == "Page 1: ì§€ì‹ì •ë³´ ìƒì„±":
    st.switch_page("pages/Knowledge_1Generator.py")
    # st.title("ğŸ—„ï¸ DB Schema to RAG Knowledge Generator")
elif page == "Page 2: ì§€ì‹ì •ë³´ ì„ë² ë”©":
    # st.title("ì¸ë±ìŠ¤ ìƒì„±")
    st.switch_page("pages/Knowledge_2Embedding.py")
elif page == "Page 3: ì§ˆë¬¸ ë° ê²€ìƒ‰" :
    st.title("ğŸ” RAG ì‹œìŠ¤í…œ - Azure AI Search")
    st.markdown("Azure AI Searchì— ì €ì¥ëœ ì§€ì‹ì„ í™œìš©í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
    # st.switch_page("pages/User_Question.py")








# ê²€ìƒ‰ ë° ë‹µë³€ ì„¤ì • (ë©”ë‰´ë°” ì œê±° / ì¸ë±ìŠ¤ ì„¤ì •í•˜ëŠ” ë¶€ë¶„ ì¶”ê°€ )

    # k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜", min_value=1, max_value=20, value=5)
    # search_type = st.selectbox("ê²€ìƒ‰ ìœ í˜•", ["similarity", "mmr", "similarity_score_threshold"])
    
    # if search_type == "similarity_score_threshold":
    #     score_threshold = st.slider("ìœ ì‚¬ë„ ì„ê³„ê°’", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    
    # temperature = st.slider("ë‹µë³€ ì°½ì˜ì„±", min_value=0.0, max_value=1.0, value=0.3, step=0.1)
    # max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", min_value=100, max_value=2000, value=1000, step=100)
st.subheader("ğŸ“Š ì¸ë±ìŠ¤ ì„¤ì •")
try:
# SearchIndexClient ìƒì„±
    credential = AzureKeyCredential(search_key)
    client = SearchIndexClient(endpoint=search_endpoint, credential=credential)
    indexes = client.list_indexes()
    name_box = []
    if indexes :
        for index in indexes:
            name_box.append(index.name)
    st.subheader("index ì„¤ì •")
    selected_selectbox = st.selectbox("í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", name_box)
    st.write("ì„ íƒëœ í•­ëª©:", selected_selectbox)
    st.divider()
    index_name = selected_selectbox
except Exception as e:
    print(f"Error: {e}")



# ì´ˆê¸°í™” í•¨ìˆ˜
@st.cache_resource
def initialize_rag_system(llm_api_key,llm_endpoint,llm_api_version,llm_deployment,embedding_deployment,search_endpoint,search_key,
                          index_name,embedding_endpoint,embedding_api_key):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    try:

        # Azure OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment=embedding_deployment,
            #openai_api_version=llm_api_version,
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key
        )
        
        # Azure AI Search ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        vector_store = AzureSearch(
            azure_search_endpoint=search_endpoint,
            azure_search_key=search_key,
            index_name=index_name,
            embedding_function=embeddings,
            search_type='hybrid'
        )
        
        # Azure OpenAI LLM ì´ˆê¸°í™”
        llm = AzureOpenAI(
            # deployment_name=llm_deployment,
            # model_name = "gpt-4o",
            api_version=llm_api_version,
            azure_endpoint=llm_endpoint,
            api_key=llm_api_key,
            #temperature=temperature,
            #max_tokens=max_tokens
        )
        # search í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        search_client = SearchClient(
            endpoint=search_endpoint,
            index_name=index_name,
            credential=AzureKeyCredential(search_key)
        )
        
        return vector_store, llm, embeddings, search_client
        
    except Exception as e:
        st.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None, None

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
def get_prompt_template():
    template = """
    ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ì»¨í…ìŠ¤íŠ¸ì— ë‹µì´ ì—†ìœ¼ë©´ "ì œê³µëœ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.

    ì»¨í…ìŠ¤íŠ¸:
    {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€ì„ ì‘ì„±í•  ë•Œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
    1. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”
    2. ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
    3. í•„ìš”í•˜ë©´ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”
    4. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”

    ë‹µë³€:
    """
    return PromptTemplate(template=template, input_variables=["context", "question"])

def generate_answer(llm, prompt):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
    try:
        response = llm.invoke(prompt)
        return response
    except Exception as e:
        st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    # í•„ìˆ˜ ì„¤ì • í™•ì¸
    if not all([llm_endpoint, llm_api_key, search_endpoint, search_key, index_name]):
        st.warning("âš ï¸ ëª¨ë“  í•„ìˆ˜ ì„¤ì •ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    vector_store, llm, embeddings, search_client = initialize_rag_system(llm_api_key,llm_endpoint,llm_api_version,llm_deployment,embedding_deployment,search_endpoint,search_key,
                          index_name,embedding_endpoint,embedding_api_key)
    if vector_store is None or llm is None:
        st.error("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    

    
    # ì§ˆì˜ì‘ë‹µ ì„¹ì…˜
    st.header("ğŸ’¬ ì§ˆì˜ì‘ë‹µ")
    
    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_input("ğŸ’­ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: íšŒì‚¬ì˜ ì£¼ìš” ì œí’ˆì€ ë¬´ì—‡ì¸ê°€ìš”?")
    
    col1, col2 = st.columns([1, 4])
    
    with col1:
        search_button = st.button("ğŸ” ê²€ìƒ‰", type="primary")
    
    with col2:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"):
            if 'chat_history' in st.session_state:
                del st.session_state['chat_history']
            st.rerun()
    
    # ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
    if search_button and question:
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            try:   
                # ê²€ìƒ‰ ì„¤ì •
                # search_kwargs = {"k": k}
                # if search_type == "similarity_score_threshold":
                    # search_kwargs["score_threshold"] = score_threshold
                
                # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                # if search_type == "mmr":
                # st.write("test before")
                # retrieved_docs = vector_store.max_marginal_relevance_search(
                #     question, k=1
                #     )
                retrieved_docs = vector_store.similarity_search(
                    question, k=1
                    )
                # st.write("test after")
                # # elif search_type == "similarity_score_threshold":
                #     retrieved_docs = vector_store.similarity_search_with_relevance_scores(
                #         question, k=k
                #     )
                #     # ì„ê³„ê°’ í•„í„°ë§
                #     retrieved_docs = [doc for doc, score in retrieved_docs if score >= score_threshold]
                # else:
                #     retrieved_docs = vector_store.similarity_search(question, k=k)
                
                # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
                st.subheader("ğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ")
                
                if retrieved_docs:
                    for i, doc in enumerate(retrieved_docs[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                        with st.expander(f"ë¬¸ì„œ {i+1} (ì¶œì²˜: {doc.metadata.get('source', 'Unknown')})"):
                            st.write(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                            st.json(doc.metadata)
                    
                    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # í”„ë¡¬í”„íŠ¸ ìƒì„±
                    prompt_template = get_prompt_template()
                    prompt = prompt_template.format(context=context, question=question)
                    
                    # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
                    st.subheader("ğŸ¤– AI ë‹µë³€")
                    
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        try:
                            # response = llm.invoke(prompt)

                            response_b = llm.chat.completions.create(
                                model = llm_deployment,
                                messages= [
                                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„°ë² ì´ìŠ¤ ì „ë¬¸ê°€ì´ë©°, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ëŠ” ì¿¼ë¦¬ë¬¸ì„ ì‘ì„±í•´ì£¼ëŠ” ê²ƒì„ ì „ë¬¸ìœ¼ë¡œ í•œë‹¤."},
                                    {"role": "user", "content": prompt}
                                ],
                                temperature=0.7,
                                max_tokens=8000
                            )

                            response = response_b.choices[0].message.content

                            st.write(response)
                            
                            # ëŒ€í™” ê¸°ë¡ ì €ì¥
                            if 'chat_history' not in st.session_state:
                                st.session_state['chat_history'] = []
                            
                            st.session_state['chat_history'].append({
                                "question": question,
                                "answer": response,
                                "retrieved_docs": len(retrieved_docs),
                                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),      
                                "index_name": index_name          
                            })
                            
                        except Exception as e:
                            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                
                else:
                    st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì„¤ì •ì„ ì¡°ì •í•´ë³´ì„¸ìš”.")
                    
            except Exception as e:
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    


    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    if 'chat_history' in st.session_state and st.session_state['chat_history']:
        st.divider()
        st.header(f"ğŸ“‹ ëŒ€í™” ê¸°ë¡")
        # st.write(f"í˜„ì¬ index : {index_name}")
        # st.write(st.session_state['chat_history'])

        for i, chat in enumerate(reversed(st.session_state['chat_history'])):
            with st.expander(f"ğŸ’­ {chat['question'][:50]}... ({chat['timestamp']})"):
                st.write(index_name)                
                st.write("**ì§ˆë¬¸:**", chat['question'])
                st.write("**ë‹µë³€:**", chat['answer'])
                st.write(f"**ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:** {chat['retrieved_docs']}")
                
if __name__ == "__main__":
    main()