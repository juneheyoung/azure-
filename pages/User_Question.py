import streamlit as st
import os
from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import AzureSearch
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
#from langchain.callbacks import StreamlitCallbackHandler
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain.schema import Document
import json
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
from azure.search.documents.indexes import SearchIndexClient
from datetime import datetime
from azure.storage.blob import BlobServiceClient

#####

load_dotenv()

llm_api_key = os.getenv("LLM_API_KEY")    
llm_endpoint = os.getenv("LLM_ENDPOINT")  
llm_api_version = os.getenv("LLM_API_VERSION")  
llm_deployment_name = os.getenv("LLM_DEPLOYMENT_NAME")

llm_deployment = os.getenv("LLM_DEPLOYMENT_NAME")

embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")
embedding_endpoint = os.getenv("EMBEDDING_ENDPOINT")
embedding_api_key = os.getenv("EMBEDDING_API_KEY")
search_endpoint = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
search_key = os.getenv("AZURE_AI_SERACH_KEY")


st.markdown("""
<style>
    [data-testid="stSidebarNav"] {
        display: none !important;
    }
    .css-1d391kg {
        display: none !important;
    }
    .css-1rs6os {
        display: none !important;
    }
    .css-17ziqus {
        display: none !important;
    }
</style>
""", unsafe_allow_html=True)

# Streamlit ÌéòÏù¥ÏßÄ ÏÑ§Ï†ï
st.set_page_config(
    page_title="RAG ÏãúÏä§ÌÖú - Azure AI Search",
    page_icon="üîç",
    layout="wide"
)


# ÏÇ¨Ïù¥ÎìúÎ∞î ÏÑ§Ï†ï
page = st.sidebar.selectbox(
    "ÌéòÏù¥ÏßÄ ÏÑ†ÌÉù",
    ["Î©îÏù∏ ÌéòÏù¥ÏßÄ", "Page 1: ÏßÄÏãùÏ†ïÎ≥¥ ÏÉùÏÑ±", "Page 2: ÏßÄÏãùÏ†ïÎ≥¥ Ï†ÄÏû•", "Page 3: ÏßàÎ¨∏ Î∞è Í≤ÄÏÉâ"],index=3
    )
st.sidebar.markdown("### üìä ÏãúÏä§ÌÖú ÏÉÅÌÉú")
# st.sidebar.info("‚úÖ ÏãúÏä§ÌÖú Ï†ïÏÉÅ ÏûëÎèô Ï§ë")
st.sidebar.markdown(f"**ÌòÑÏû¨ ÏãúÍ∞Ñ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# Î©îÏù∏ ÌéòÏù¥ÏßÄ
if page == "Î©îÏù∏ ÌéòÏù¥ÏßÄ":
    # Ìó§Îçî
    st.title("üß† ÏßÄÏãù Ï†ïÎ≥¥ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú")
    # st.markdown("### Ìö®Ïú®Ï†ÅÏù∏ ÏßÄÏãù Ï†ïÎ≥¥ ÏÉùÏÑ±, ÏûÑÎ≤†Îî©, Í≤ÄÏÉâÏùÑ ÏúÑÌïú ÌÜµÌï© ÌîåÎû´Ìèº")
    st.switch_page("./main.py")


elif page == "Page 1: ÏßÄÏãùÏ†ïÎ≥¥ ÏÉùÏÑ±":
    st.switch_page("pages/Knowledge_1Generator.py")
    # st.title("üóÑÔ∏è DB Schema to RAG Knowledge Generator")
elif page == "Page 2: ÏßÄÏãùÏ†ïÎ≥¥ ÏûÑÎ≤†Îî©":
    # st.title("Ïù∏Îç±Ïä§ ÏÉùÏÑ±")
    st.switch_page("pages/Knowledge_2Embedding.py")
elif page == "Page 3: ÏßàÎ¨∏ Î∞è Í≤ÄÏÉâ" :
    st.title("üîç RAG ÏãúÏä§ÌÖú - Azure AI Search")
    st.markdown("Azure AI SearchÏóê Ï†ÄÏû•Îêú ÏßÄÏãùÏùÑ ÌôúÏö©Ìïú ÏßàÏùòÏùëÎãµ ÏãúÏä§ÌÖú")
    # st.switch_page("pages/User_Question.py")








# Í≤ÄÏÉâ Î∞è ÎãµÎ≥Ä ÏÑ§Ï†ï (Î©îÎâ¥Î∞î Ï†úÍ±∞ / Ïù∏Îç±Ïä§ ÏÑ§Ï†ïÌïòÎäî Î∂ÄÎ∂Ñ Ï∂îÍ∞Ä )

    # k = st.slider("Í≤ÄÏÉâÌï† Î¨∏ÏÑú Ïàò", min_value=1, max_value=20, value=5)
    # search_type = st.selectbox("Í≤ÄÏÉâ Ïú†Ìòï", ["similarity", "mmr", "similarity_score_threshold"])
    
    # if search_type == "similarity_score_threshold":
    #     score_threshold = st.slider("Ïú†ÏÇ¨ÎèÑ ÏûÑÍ≥ÑÍ∞í", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    
    # temperature = st.slider("ÎãµÎ≥Ä Ï∞ΩÏùòÏÑ±", min_value=0.0, max_value=1.0, value=0.3, step=0.1)
    # max_tokens = st.slider("ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò", min_value=100, max_value=2000, value=1000, step=100)
st.subheader("üìä Ïù∏Îç±Ïä§ ÏÑ§Ï†ï")
try:
# SearchIndexClient ÏÉùÏÑ±
    credential = AzureKeyCredential(search_key)
    client = SearchIndexClient(endpoint=search_endpoint, credential=credential)
    indexes = client.list_indexes()
    name_box = []
    if indexes :
        for index in indexes:
            name_box.append(index.name)
    st.subheader("index ÏÑ§Ï†ï")
    selected_selectbox = st.selectbox("Ìï≠Î™©ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî:", name_box)
    st.write("ÏÑ†ÌÉùÎêú Ìï≠Î™©:", selected_selectbox)
    st.divider()
    index_name = selected_selectbox
except Exception as e:
    print(f"Error: {e}")



# Ï¥àÍ∏∞Ìôî Ìï®Ïàò
@st.cache_resource
def initialize_rag_system(llm_api_key,llm_endpoint,llm_api_version,llm_deployment,embedding_deployment,search_endpoint,search_key,
                          index_name,embedding_endpoint,embedding_api_key):
    """RAG ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî"""
    try:

        # Azure OpenAI ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment=embedding_deployment,
            #openai_api_version=llm_api_version,
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key
        )
        
        # Azure AI Search Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ Ï¥àÍ∏∞Ìôî
        vector_store = AzureSearch(
            azure_search_endpoint=search_endpoint,
            azure_search_key=search_key,
            index_name=index_name,
            embedding_function=embeddings,
            search_type='hybrid'
        )
        
        # Azure OpenAI LLM Ï¥àÍ∏∞Ìôî
        llm = AzureOpenAI(
            # deployment_name=llm_deployment,
            # model_name = "gpt-4o",
            api_version=llm_api_version,
            azure_endpoint=llm_endpoint,
            api_key=llm_api_key,
            #temperature=temperature,
            #max_tokens=max_tokens
        )
        # search ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
        search_client = SearchClient(
            endpoint=search_endpoint,
            index_name=index_name,
            credential=AzureKeyCredential(search_key)
        )
        
        return vector_store, llm, embeddings, search_client
        
    except Exception as e:
        st.error(f"RAG ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
        return None, None, None

# ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø
def get_prompt_template():
    template = """
    Îã§Ïùå Ïª®ÌÖçÏä§Ìä∏Î•º Î∞îÌÉïÏúºÎ°ú ÏßàÎ¨∏Ïóê ÎãµÌïòÏÑ∏Ïöî. Ïª®ÌÖçÏä§Ìä∏Ïóê ÎãµÏù¥ ÏóÜÏúºÎ©¥ "Ï†úÍ≥µÎêú Ï†ïÎ≥¥Î°úÎäî ÎãµÎ≥ÄÌï† Ïàò ÏóÜÏäµÎãàÎã§"ÎùºÍ≥† ÎßêÌïòÏÑ∏Ïöî.

    Ïª®ÌÖçÏä§Ìä∏:
    {context}

    ÏßàÎ¨∏: {question}

    ÎãµÎ≥ÄÏùÑ ÏûëÏÑ±Ìï† Îïå Îã§Ïùå ÏßÄÏπ®ÏùÑ Îî∞Î•¥ÏÑ∏Ïöî:
    1. Ïª®ÌÖçÏä§Ìä∏Ïùò Ï†ïÎ≥¥Î•º Ï†ïÌôïÌïòÍ≤å ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî
    2. Î™ÖÌôïÌïòÍ≥† Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú ÎãµÎ≥ÄÌïòÏÑ∏Ïöî
    3. ÌïÑÏöîÌïòÎ©¥ Îã®Í≥ÑÎ≥ÑÎ°ú ÏÑ§Î™ÖÌïòÏÑ∏Ïöî
    4. ÌïúÍµ≠Ïñ¥Î°ú ÎãµÎ≥ÄÌïòÏÑ∏Ïöî

    ÎãµÎ≥Ä:
    """
    return PromptTemplate(template=template, input_variables=["context", "question"])

def generate_answer(llm, prompt):
    """LLMÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÎãµÎ≥Ä ÏÉùÏÑ±"""
    try:
        response = llm.invoke(prompt)
        return response
    except Exception as e:
        st.error(f"ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
        return None


# Î©îÏù∏ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò
def main():
    # ÌïÑÏàò ÏÑ§Ï†ï ÌôïÏù∏
    if not all([llm_endpoint, llm_api_key, search_endpoint, search_key, index_name]):
        st.warning("‚ö†Ô∏è Î™®Îì† ÌïÑÏàò ÏÑ§Ï†ïÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.")
        return
    
    # RAG ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî
    vector_store, llm, embeddings, search_client = initialize_rag_system(llm_api_key,llm_endpoint,llm_api_version,llm_deployment,embedding_deployment,search_endpoint,search_key,
                          index_name,embedding_endpoint,embedding_api_key)
    if vector_store is None or llm is None:
        st.error("RAG ÏãúÏä§ÌÖú Ï¥àÍ∏∞ÌôîÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.")
        return
    

    
    # ÏßàÏùòÏùëÎãµ ÏÑπÏÖò
    st.header("üí¨ ÏßàÏùòÏùëÎãµ")
    
    # ÏßàÎ¨∏ ÏûÖÎ†•
    question = st.text_input("üí≠ ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî:", placeholder="Ïòà: ÌöåÏÇ¨Ïùò Ï£ºÏöî Ï†úÌíàÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?")
    
    col1, col2 = st.columns([1, 4])
    
    with col1:
        search_button = st.button("üîç Í≤ÄÏÉâ", type="primary")
    
    with col2:
        if st.button("üóëÔ∏è ÎåÄÌôî Í∏∞Î°ù ÏÇ≠Ï†ú"):
            if 'chat_history' in st.session_state:
                del st.session_state['chat_history']
            st.rerun()
    
    # Í≤ÄÏÉâ Î∞è ÎãµÎ≥Ä ÏÉùÏÑ±
    if search_button and question:
        with st.spinner("Í≤ÄÏÉâ Ï§ë..."):
            try:   
                # Í≤ÄÏÉâ ÏÑ§Ï†ï
                # search_kwargs = {"k": k}
                # if search_type == "similarity_score_threshold":
                    # search_kwargs["score_threshold"] = score_threshold
                
                # Î≤°ÌÑ∞ Ïä§ÌÜ†Ïñ¥ÏóêÏÑú Í¥ÄÎ†® Î¨∏ÏÑú Í≤ÄÏÉâ
                # if search_type == "mmr":
                # st.write("test before")
                # retrieved_docs = vector_store.max_marginal_relevance_search(
                #     question, k=1
                #     )
                retrieved_docs = vector_store.similarity_search(
                    question, k=1
                    )
                # st.write("test after")
                # # elif search_type == "similarity_score_threshold":
                #     retrieved_docs = vector_store.similarity_search_with_relevance_scores(
                #         question, k=k
                #     )
                #     # ÏûÑÍ≥ÑÍ∞í ÌïÑÌÑ∞ÎßÅ
                #     retrieved_docs = [doc for doc, score in retrieved_docs if score >= score_threshold]
                # else:
                #     retrieved_docs = vector_store.similarity_search(question, k=k)
                
                # Í≤ÄÏÉâ Í≤∞Í≥º ÌëúÏãú
                st.subheader("üìö Í≤ÄÏÉâÎêú Í¥ÄÎ†® Î¨∏ÏÑú")
                
                if retrieved_docs:
                    for i, doc in enumerate(retrieved_docs[:3]):  # ÏµúÎåÄ 3Í∞úÎßå ÌëúÏãú
                        with st.expander(f"Î¨∏ÏÑú {i+1} (Ï∂úÏ≤ò: {doc.metadata.get('source', 'Unknown')})"):
                            st.write(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                            st.json(doc.metadata)
                    
                    # Ïª®ÌÖçÏä§Ìä∏ ÏÉùÏÑ±
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ±
                    prompt_template = get_prompt_template()
                    prompt = prompt_template.format(context=context, question=question)
                    
                    # LLMÏúºÎ°ú ÎãµÎ≥Ä ÏÉùÏÑ±
                    st.subheader("ü§ñ AI ÎãµÎ≥Ä")
                    
                    with st.spinner("ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë..."):
                        try:
                            # response = llm.invoke(prompt)

                            response_b = llm.chat.completions.create(
                                model = llm_deployment,
                                messages= [
                                    {"role": "system", "content": "ÎãπÏã†ÏùÄ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ÑÎ¨∏Í∞ÄÏù¥Î©∞, ÏÇ¨Ïö©ÏûêÏùò ÏßàÎ¨∏Ïóê ÎßûÎäî ÏøºÎ¶¨Î¨∏ÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÎäî Í≤ÉÏùÑ Ï†ÑÎ¨∏ÏúºÎ°ú ÌïúÎã§."},
                                    {"role": "user", "content": prompt}
                                ],
                                temperature=0.7,
                                max_tokens=8000
                            )

                            response = response_b.choices[0].message.content

                            st.write(response)
                            
                            # ÎåÄÌôî Í∏∞Î°ù Ï†ÄÏû•
                            if 'chat_history' not in st.session_state:
                                st.session_state['chat_history'] = []
                            
                            st.session_state['chat_history'].append({
                                "question": question,
                                "answer": response,
                                "retrieved_docs": len(retrieved_docs),
                                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),      
                                "index_name": index_name          
                            })
                            
                        except Exception as e:
                            st.error(f"ÎãµÎ≥Ä ÏÉùÏÑ± Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
                
                else:
                    st.warning("Í¥ÄÎ†® Î¨∏ÏÑúÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Í≤ÄÏÉâ ÏÑ§Ï†ïÏùÑ Ï°∞Ï†ïÌï¥Î≥¥ÏÑ∏Ïöî.")
                    
            except Exception as e:
                st.error(f"Í≤ÄÏÉâ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}")
    


    
    # ÎåÄÌôî Í∏∞Î°ù ÌëúÏãú
    if 'chat_history' in st.session_state and st.session_state['chat_history']:
        st.divider()
        st.header(f"üìã ÎåÄÌôî Í∏∞Î°ù")
        # st.write(f"ÌòÑÏû¨ index : {index_name}")
        # st.write(st.session_state['chat_history'])

        for i, chat in enumerate(reversed(st.session_state['chat_history'])):
            with st.expander(f"üí≠ {chat['question'][:50]}... ({chat['timestamp']})"):
                st.write(index_name)                
                st.write("**ÏßàÎ¨∏:**", chat['question'])
                st.write("**ÎãµÎ≥Ä:**", chat['answer'])
                st.write(f"**Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò:** {chat['retrieved_docs']}")
                
if __name__ == "__main__":
    main()