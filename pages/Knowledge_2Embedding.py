import os
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
import streamlit as st
import json

import tempfile
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document

from azure.search.documents.indexes import SearchIndexClient
from azure.core.credentials import AzureKeyCredential
import pandas as pd 
from azure.search.documents import SearchClient

load_dotenv()

llm_api_key = os.getenv("LLM_API_KEY")
llm_endpoint = os.getenv("LLM_ENDPOINT")
llm_api_version = os.getenv("LLM_API_VERSION")
llm_deployment_name = os.getenv("LLM_DEPLOYMENT_NAME")
ai_search_endpoint = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
ai_search_api_key = os.getenv("AZURE_AI_SERACH_KEY")
azure_embedding_endpoint = os.getenv("EMBEDDING_ENDPOINT")
azure_embedding_api_key = os.getenv("EMBEDDING_API_KEY")
azure_embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")

# Option 2: Use AzureOpenAIEmbeddings with an Azure account
embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(
    azure_deployment=azure_embedding_deployment,
    #openai_api_version=azure_openai_api_version,
    azure_endpoint=azure_embedding_endpoint,
    api_key=azure_embedding_api_key,
)

# ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ
tab1, tab2 = st.tabs(["ğŸ“‹ ì¸ë±ìŠ¤ ëª©ë¡", "â• ìƒˆ ì¸ë±ìŠ¤ ìƒì„±"])        
with tab1:
    
    st.header("ì¸ë±ìŠ¤ ëª©ë¡")
            
    try:
        # SearchIndexClient ìƒì„±
        credential = AzureKeyCredential(ai_search_api_key)
        client = SearchIndexClient(endpoint=ai_search_endpoint, credential=credential)
        #indexes = list(client.list_indexes())
        indexes = client.list_indexes()
        name_box = []
        if indexes :
            for index in indexes:
                name_box.append(index.name)
        
            
            st.subheader("index ì„¤ì •")
            selected_selectbox = st.selectbox("í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", name_box)
            st.write("ì„ íƒëœ í•­ëª©:", selected_selectbox)
            st.divider()
            new_index_name = selected_selectbox
        
            if new_index_name != None :
                vector_store: AzureSearch = AzureSearch(
                azure_search_endpoint=ai_search_endpoint,
                azure_search_key=ai_search_api_key,
                index_name=new_index_name,
                embedding_function=embeddings.embed_query,
                )
                search_client = SearchClient(
                    endpoint=ai_search_endpoint,
                    index_name=new_index_name,
                    credential=AzureKeyCredential(ai_search_api_key)
                )

                st.subheader("ğŸ“ TXT íŒŒì¼ ì—…ë¡œë“œ")
                uploaded_file = st.file_uploader("TXT íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['txt'])

                if uploaded_file is not None:
                    try:
                        # íŒŒì¼ ë‚´ìš© ì§ì ‘ ì½ê¸°
                        content = uploaded_file.read().decode('utf-8')
                        
                        # Document ê°ì²´ ì§ì ‘ ìƒì„±
                        document = Document(
                            page_content=content,
                            metadata={"source": uploaded_file.name, "type": "txt"}
                        )
                        st.success("TXT íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.write(f"íŒŒì¼ í¬ê¸°: {len(content)} ë¬¸ì")
                        
                        # í…ìŠ¤íŠ¸ ë¶„í• 
                        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
                        docs = text_splitter.split_documents([document])
                        
                        result = search_client.search("*", include_total_count=True, top=0)
                        document_count = result.get_count()
                        
                        st.success(f"ì´ {len(docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {document_count}")
                        # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
                        vector_store.add_documents(documents=docs)
                        st.success("ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                    except Exception as e:
                        st.error(f"TXT íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    
    except Exception as e:
        print(f"Error: {e}")

with tab2:

    st.header("ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
    
    with st.form("new_index_form"):
        st.subheader("ê¸°ë³¸ ì •ë³´")
        
        # ì¸ë±ìŠ¤ ì´ë¦„
        new_index_name = st.text_input(
            "ì¸ë±ìŠ¤ ì´ë¦„ *",
            placeholder="ì˜ˆ: my-search-index",
            help="?ì˜ë¬¸ ì†Œë¬¸ì, ìˆ«ì, í•˜ì´í”ˆë§Œ ì‚¬ìš© ê°€ëŠ¥"
        ) 
        submitted = st.form_submit_button("ì¸ë±ìŠ¤ ìƒì„±", type="primary")       
        if submitted and new_index_name:
            vector_store: AzureSearch = AzureSearch(
            azure_search_endpoint=ai_search_endpoint,
            azure_search_key=ai_search_api_key,
            index_name=new_index_name,
            embedding_function=embeddings.embed_query,
            )

            st.subheader("ğŸ“ TXT íŒŒì¼ ì—…ë¡œë“œ")
            uploaded_file = st.file_uploader("TXT íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['txt'])

            if uploaded_file is not None:
                try:
                    # íŒŒì¼ ë‚´ìš© ì§ì ‘ ì½ê¸°
                    content = uploaded_file.read().decode('utf-8')
                    
                    # Document ê°ì²´ ì§ì ‘ ìƒì„±
                    document = Document(
                        page_content=content,
                        metadata={"source": uploaded_file.name, "type": "txt"}
                    )
                    st.success("TXT íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.write(f"íŒŒì¼ í¬ê¸°: {len(content)} ë¬¸ì")
                    
                    # í…ìŠ¤íŠ¸ ë¶„í• 
                    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
                    docs = text_splitter.split_documents([document])
                    
                    st.success(f"ì´ {len(docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ìƒì„±(ì¶”ê°€)ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
                    vector_store.add_documents(documents=docs)
                    st.success("ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                except Exception as e:
                    st.error(f"TXT íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")








### index ì‚­ì œ 
### input data - type  ì§€ì •í•˜ê¸° 
### ë¬¸ì„œê°¯ìˆ˜ ì›ë˜ê±° í¬í•¨í•´ì„œ ì´ê°œìˆ˜ë¡œ ë‚˜ì˜¤ê²Œ í•˜ê¸° 
