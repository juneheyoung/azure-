import os
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
import streamlit as st
import json

import tempfile
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document

from azure.search.documents.indexes import SearchIndexClient
from azure.core.credentials import AzureKeyCredential
import pandas as pd 
from azure.search.documents import SearchClient
from datetime import datetime

load_dotenv()

#í™˜ê²½ë³€ìˆ˜
llm_api_key = os.getenv("LLM_API_KEY")
llm_endpoint = os.getenv("LLM_ENDPOINT")
llm_api_version = os.getenv("LLM_API_VERSION")
llm_deployment_name = os.getenv("LLM_DEPLOYMENT_NAME")
ai_search_endpoint = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
ai_search_api_key = os.getenv("AZURE_AI_SERACH_KEY")
azure_embedding_endpoint = os.getenv("EMBEDDING_ENDPOINT")
azure_embedding_api_key = os.getenv("EMBEDDING_API_KEY")
azure_embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.markdown("""
<style>
    [data-testid="stSidebarNav"] {
        display: none !important;
    }
    .css-1d391kg {
        display: none !important;
    }
    .css-1rs6os {
        display: none !important;
    }
    .css-17ziqus {
        display: none !important;
    }
</style>
""", unsafe_allow_html=True)

page = st.sidebar.selectbox(
    "í˜ì´ì§€ ì„ íƒ",
    ["ë©”ì¸ í˜ì´ì§€", "Page 1: ì§€ì‹ì •ë³´ ìƒì„±", "Page 2: ì§€ì‹ì •ë³´ ì„ë² ë”©", "Page 3: ì§ˆë¬¸ ë° ê²€ìƒ‰",],index=2
    )
st.sidebar.markdown("### ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
# st.sidebar.info("âœ… ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ ì¤‘")
st.sidebar.markdown(f"**í˜„ì¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# ë©”ì¸ í˜ì´ì§€
if page == "ë©”ì¸ í˜ì´ì§€":
    # í—¤ë”
    st.title("ğŸ§  ì§€ì‹ ì •ë³´ ê´€ë¦¬ ì‹œìŠ¤í…œ")
    # st.markdown("### íš¨ìœ¨ì ì¸ ì§€ì‹ ì •ë³´ ìƒì„±, ì„ë² ë”©, ê²€ìƒ‰ì„ ìœ„í•œ í†µí•© í”Œë«í¼")
    st.switch_page("./main.py")


elif page == "Page 1: ì§€ì‹ì •ë³´ ìƒì„±":
    st.switch_page("pages/Knowledge_1Generator.py")
    # st.title("ğŸ—„ï¸ DB Schema to RAG Knowledge Generator")
elif page == "Page 2: ì§€ì‹ì •ë³´ ì„ë² ë”©":
    st.title("ì¸ë±ìŠ¤ ìƒì„±")
    # st.switch_page("pages/Knowledge_2Embedding.py")
elif page == "Page 3: ì§ˆë¬¸ ë° ê²€ìƒ‰" :
    st.switch_page("pages/User_Question.py")








# Option : Use AzureOpenAIEmbeddings with an Azure account
embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(
    azure_deployment=azure_embedding_deployment,
    #openai_api_version=azure_openai_api_version,
    azure_endpoint=azure_embedding_endpoint,
    api_key=azure_embedding_api_key,
)

# ì¸ë±ìŠ¤ ëª©ë¡ ì¡°íšŒ
tab1, tab2 = st.tabs(["ğŸ“‹ ì¸ë±ìŠ¤ ëª©ë¡", "â• ìƒˆ ì¸ë±ìŠ¤ ìƒì„±"])        
with tab1:
    
    st.header("ì¸ë±ìŠ¤ ëª©ë¡")
            
    try:
        # SearchIndexClient ìƒì„±
        credential = AzureKeyCredential(ai_search_api_key)
        client = SearchIndexClient(endpoint=ai_search_endpoint, credential=credential)
        #indexes = list(client.list_indexes())
        indexes = client.list_indexes()
        name_box = []

        if indexes :
            for index in indexes:
                name_box.append(index.name)
        
            
            st.subheader("index ì„¤ì •")
            selected_selectbox = st.selectbox("í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", name_box)
            st.write("ì„ íƒëœ í•­ëª©:", selected_selectbox)
            st.divider()
            new_index_name = selected_selectbox
        
            if new_index_name != None :
                vector_store: AzureSearch = AzureSearch(
                azure_search_endpoint=ai_search_endpoint,
                azure_search_key=ai_search_api_key,
                index_name=new_index_name,
                embedding_function=embeddings.embed_query,
                )
                search_client = SearchClient(
                    endpoint=ai_search_endpoint,
                    index_name=new_index_name,
                    credential=AzureKeyCredential(ai_search_api_key)
                )

                st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
                uploaded_file = st.file_uploader("ì—…ë¡œë“œ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['txt','md','json'])

                if uploaded_file is not None:
                    file_extension = uploaded_file.name.split('.')[-1].lower()
                    if file_extension == 'txt':
                        # TXT íŒŒì¼ ì²˜ë¦¬
                        content = uploaded_file.read().decode('utf-8')
                    elif file_extension == 'json':
                        # JSON íŒŒì¼ ì²˜ë¦¬                  
                        # # 
                        # try:
                        #     data = json.loads(uploaded_file)
                        # except json.JSONDecodeError as e:
                        #     st.write(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                        content = uploaded_file.read().decode('utf-8')      
                        # content = json.load(uploaded_file)
                    elif file_extension == 'md':
                        # MD íŒŒì¼ ì²˜ë¦¬
                        content = uploaded_file.read().decode('utf-8')
                    try:
                        # íŒŒì¼ ë‚´ìš© ì§ì ‘ ì½ê¸°
                        #                     
                        # Document ê°ì²´ ì§ì ‘ ìƒì„±
                        document = Document(
                            page_content=content,
                            metadata={"source": uploaded_file.name, "type": file_extension }
                        )
                        st.success(f"{file_extension}íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.write(f"íŒŒì¼ í¬ê¸°: {len(content)} ë¬¸ì")
                        
                        # í…ìŠ¤íŠ¸ ë¶„í• 
                        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
                        docs = text_splitter.split_documents([document])
                        
                        result = search_client.search("*", include_total_count=True, top=0)
                        document_count = result.get_count()
                        
                        st.success(f"ì´ {len(docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {document_count}")
                        # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
                        vector_store.add_documents(documents=docs)
                        st.success("ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        
                    except Exception as e:
                        st.error(f"TXT íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    
    except Exception as e:
        print(f"Error: {e}")

with tab2:

    st.header("ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
    
    with st.form("new_index_form"):
        st.subheader("ê¸°ë³¸ ì •ë³´")
        
        # ì¸ë±ìŠ¤ ì´ë¦„
        new_index_name = st.text_input(
            "ì¸ë±ìŠ¤ ì´ë¦„ *",
            placeholder="ì˜ˆ: my-search-index",
            help="?ì˜ë¬¸ ì†Œë¬¸ì, ìˆ«ì, í•˜ì´í”ˆë§Œ ì‚¬ìš© ê°€ëŠ¥"
        ) 
        submitted = st.form_submit_button("ì¸ë±ìŠ¤ ìƒì„±", type="primary")       
        if submitted and new_index_name:
            vector_store: AzureSearch = AzureSearch(
            azure_search_endpoint=ai_search_endpoint,
            azure_search_key=ai_search_api_key,
            index_name=new_index_name,
            embedding_function=embeddings.embed_query,
            )
            search_client = SearchClient(
            endpoint=ai_search_endpoint,
            index_name=new_index_name,
            credential=AzureKeyCredential(ai_search_api_key)
            )
            st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
            uploaded_file = st.file_uploader("ì—…ë¡œë“œ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['txt','md','json'])

            if uploaded_file is not None:
                st.write(uploaded_file.name)
                file_extension = uploaded_file.name.split('.')[-1].lower()
                st.write(file_extension)
                if file_extension == 'txt':
                    # TXT íŒŒì¼ ì²˜ë¦¬
                    content = uploaded_file.read().decode('utf-8')
                elif file_extension == 'json':
                    # JSON íŒŒì¼ ì²˜ë¦¬                        
                    content = json.load(uploaded_file)
                elif file_extension == 'md':
                    # MD íŒŒì¼ ì²˜ë¦¬
                    content = uploaded_file.read().decode('utf-8')
                try:
                    # íŒŒì¼ ë‚´ìš© ì§ì ‘ ì½ê¸°
                    #                     
                    # Document ê°ì²´ ì§ì ‘ ìƒì„±
                    document = Document(
                        page_content=content,
                        metadata={"source": uploaded_file.name, "type": file_extension }
                    )
                    st.success(f"{file_extension}íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.write(f"íŒŒì¼ í¬ê¸°: {len(content)} ë¬¸ì")
                    
                    # í…ìŠ¤íŠ¸ ë¶„í• 
                    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
                    docs = text_splitter.split_documents([document])
                    
                    result = search_client.search("*", include_total_count=True, top=0)
                    document_count = result.get_count()
                    
                    st.success(f"ì´ {len(docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {document_count}")
                    # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
                    vector_store.add_documents(documents=docs)
                    st.success("ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                except Exception as e:
                    st.error(f"TXT íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")








### index ì‚­ì œ 
### input data - type  ì§€ì •í•˜ê¸° 
### ë¬¸ì„œê°¯ìˆ˜ ì›ë˜ê±° í¬í•¨í•´ì„œ ì´ê°œìˆ˜ë¡œ ë‚˜ì˜¤ê²Œ í•˜ê¸° 
