import os
from langchain_community.vectorstores.azuresearch import AzureSearch
from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
import streamlit as st
import json

import tempfile
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document

load_dotenv()

azure_openai_api_key = os.getenv("OPEN_API_KEY")
azure_endpoint = os.getenv("AZURE_ENDPOINT")
azure_openai_api_version = os.getenv("API_VERSION")
azure_deployment = os.getenv("DEPLOYMENT_NAME")

vector_store_address = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
vector_store_password = os.getenv("AZURE_AI_SERACH_KEY")

azure_embedding_endpoint = os.getenv("EMBEDDING_ENDPOINT")
azure_embedding_api_key = os.getenv("EMBEDDING_API_KEY")
azure_embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")

# Option 2: Use AzureOpenAIEmbeddings with an Azure account
embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(
    azure_deployment=azure_embedding_deployment,
    #openai_api_version=azure_openai_api_version,
    azure_endpoint=azure_embedding_endpoint,
    api_key=azure_embedding_api_key,
)

index_name: str = "langchain-vector-demo"   ### index_name change ? 
vector_store: AzureSearch = AzureSearch(
    azure_search_endpoint=vector_store_address,
    azure_search_key=vector_store_password,
    index_name=index_name,
    embedding_function=embeddings.embed_query,
)

### json ì¶”ê°€ / ë‹¤ë¥¸ê²ƒ ì¶”ê°€ /// 

# file upload í•˜ê¸° 

# schema_data = None

# st.subheader("ğŸ“ TXT íŒŒì¼ ì—…ë¡œë“œ")
# uploaded_file = st.file_uploader("TXT íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['txt'])
# if uploaded_file is not None:
#     try:
#         schema_data = TextLoader(uploaded_file, encoding='utf-8')
#         st.success("TXT íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
#         # st.text(schema_data)
#     except Exception as e:
#         st.error(f"TXT íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    

    
#     loader = uploaded_file

#     documents = loader.load()
#     text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
#     docs = text_splitter.split_documents(documents)

#     vector_store.add_documents(documents=docs)

st.subheader("ğŸ“ TXT íŒŒì¼ ì—…ë¡œë“œ")
uploaded_file = st.file_uploader("TXT íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", type=['txt'])

if uploaded_file is not None:
    try:
        # íŒŒì¼ ë‚´ìš© ì§ì ‘ ì½ê¸°
        content = uploaded_file.read().decode('utf-8')
        
        # Document ê°ì²´ ì§ì ‘ ìƒì„±
        document = Document(
            page_content=content,
            metadata={"source": uploaded_file.name, "type": "txt"}
        )
        
        st.success("TXT íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.write(f"íŒŒì¼ í¬ê¸°: {len(content)} ë¬¸ì")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
        docs = text_splitter.split_documents([document])
        
        st.success(f"ì´ {len(docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
        vector_store.add_documents(documents=docs)
        st.success("ë¬¸ì„œê°€ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        st.error(f"TXT íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")