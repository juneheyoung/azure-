import os
import streamlit as st 
from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings
# from langchain_community.vectorstores import AzureSearch
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
#from langchain.callbacks import StreamlitCallbackHandler
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain.schema import Document
import json
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
from azure.search.documents.indexes import SearchIndexClient
from langchain_community.vectorstores.azuresearch import AzureSearch
from azure.search.documents.models import VectorizableTextQuery

load_dotenv()
llm_api_key = os.getenv("LLM_API_KEY")    
llm_endpoint = os.getenv("LLM_ENDPOINT")  
llm_api_version = os.getenv("LLM_API_VERSION")  
llm_deployment_name = os.getenv("LLM_DEPLOYMENT_NAME")
llm_deployment = os.getenv("LLM_DEPLOYMENT_NAME")
embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")
embedding_endpoint = os.getenv("EMBEDDING_ENDPOINT")
embedding_api_key = os.getenv("EMBEDDING_API_KEY")
search_endpoint = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
search_key = os.getenv("AZURE_AI_SERACH_KEY")



st.set_page_config(
    page_title="RAG ì‹œìŠ¤í…œ - Azure AI Search",
    page_icon="ğŸ”",
    layout="wide"
)
st.title("ğŸ” RAG ì‹œìŠ¤í…œ - Azure AI Search")
st.markdown("Azure AI Searchì— ì €ì¥ëœ ì§€ì‹ì„ í™œìš©í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

with st.sidebar.expander("ğŸ¯ ê²€ìƒ‰ ë° ë‹µë³€ ì„¤ì •"):
    try:
    # SearchIndexClient ìƒì„±
        credential = AzureKeyCredential(search_key)
        client = SearchIndexClient(endpoint=search_endpoint, credential=credential)
        indexes = client.list_indexes()
        name_box = []
        if indexes :
            for index in indexes:
                name_box.append(index.name)
        st.subheader("index ì„¤ì •")
        selected_selectbox = st.selectbox("í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:", name_box)
        st.write("ì„ íƒëœ í•­ëª©:", selected_selectbox)
        st.divider()
        index_name = selected_selectbox
    except Exception as e:
        print(f"Error: {e}")

search_client = SearchClient(endpoint=search_endpoint,
                             index_name=index_name,
                             search_type="hybrid",
                             credential=AzureKeyCredential(search_key))


openai_client = AzureOpenAI(
    api_version=llm_api_version,
    azure_endpoint=llm_endpoint,
    api_key=llm_api_key
)

def rag_query(user_question):
    # 1. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
    search_results = search_client.search(
        search_text=user_question,
        top=5,
        select=["content", "metadata"]  # í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒ
    )
    
    # 2. ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
    context = ""
    for result in search_results:
        context += f"{result['content']}\n\n"

    # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""
    ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
    
    ë¬¸ì„œ ë‚´ìš©:
    {context}
    
    ì§ˆë¬¸: {user_question}
    
    ë‹µë³€:
    """
    
    # 4. LLMì—ê²Œ ë‹µë³€ ìš”ì²­
    response = openai_client.chat.completions.create(
        model=llm_deployment,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3
    )
    
    return response.choices[0].message.content

answer = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ê¶ê¸ˆí•œ ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”...")

# ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œë§Œ ë‹µë³€ ìƒì„±
if answer:
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        try:
            llm_answer = rag_query(answer)
            st.write("### ë‹µë³€:")
            st.write(llm_answer)
        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")