import streamlit as st
import os
from langchain_openai import AzureOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import AzureSearch
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
#from langchain.callbacks import StreamlitCallbackHandler
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler
from langchain.schema import Document
import json
from dotenv import load_dotenv
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential




load_dotenv()

llm_api_key = os.getenv("LLM_API_KEY")    
llm_endpoint = os.getenv("LLM_ENDPOINT")  
llm_api_version = os.getenv("LLM_API_VERSION")  
llm_deployment_name = os.getenv("LLM_DEPLOYMENT_NAME")

llm_deployment = os.getenv("LLM_DEPLOYMENT_NAME")

embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")
embedding_endpoint = os.getenv("EMBEDDING_ENDPOINT")
embedding_api_key = os.getenv("EMBEDDING_API_KEY")
search_endpoint = os.getenv("AZURE_AI_SEARCH_ENDPOINT")
search_key = os.getenv("AZURE_AI_SERACH_KEY")
index_name = "langchain-vector-demo"


#azure_embedding_deployment = os.getenv("EMBEDDING_DEPLOYMENT")

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
search_client = SearchClient(
    endpoint=search_endpoint,
    index_name=index_name,
    credential=AzureKeyCredential(search_key)
)

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="RAG ì‹œìŠ¤í…œ - Azure AI Search",
    page_icon="ğŸ”",
    layout="wide"
)
st.title("ğŸ” RAG ì‹œìŠ¤í…œ - Azure AI Search")
st.markdown("Azure AI Searchì— ì €ì¥ëœ ì§€ì‹ì„ í™œìš©í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")



# ê²€ìƒ‰ ë° ë‹µë³€ ì„¤ì • (ë©”ë‰´ë°” ì œê±° / ì¸ë±ìŠ¤ ì„¤ì •í•˜ëŠ” ë¶€ë¶„ ì¶”ê°€ )
with st.sidebar.expander("ğŸ¯ ê²€ìƒ‰ ë° ë‹µë³€ ì„¤ì •"):
    k = st.slider("ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜", min_value=1, max_value=20, value=5)
    search_type = st.selectbox("ê²€ìƒ‰ ìœ í˜•", ["similarity", "mmr", "similarity_score_threshold"])
    
    if search_type == "similarity_score_threshold":
        score_threshold = st.slider("ìœ ì‚¬ë„ ì„ê³„ê°’", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
    
    temperature = st.slider("ë‹µë³€ ì°½ì˜ì„±", min_value=0.0, max_value=1.0, value=0.3, step=0.1)
    max_tokens = st.slider("ìµœëŒ€ í† í° ìˆ˜", min_value=100, max_value=2000, value=1000, step=100)

# ì´ˆê¸°í™” í•¨ìˆ˜
@st.cache_resource
def initialize_rag_system(llm_api_key,llm_endpoint,llm_api_version,llm_deployment,embedding_deployment,search_endpoint,search_key,
                          index_name,embedding_endpoint,embedding_api_key):
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    try:
        # Azure OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment=embedding_deployment,
            #openai_api_version=llm_api_version,
            azure_endpoint=embedding_endpoint,
            api_key=embedding_api_key
        )
        
        # Azure AI Search ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        vector_store = AzureSearch(
            azure_search_endpoint=search_endpoint,
            azure_search_key=search_key,
            index_name=index_name,
            embedding_function=embeddings,
        )
        
        # Azure OpenAI LLM ì´ˆê¸°í™”
        llm = AzureOpenAI(
            deployment_name=llm_deployment,
            # model_name = 'gpt-4.1',
            api_version=llm_api_version,
            azure_endpoint=llm_endpoint,
            api_key=llm_api_key,
            #temperature=temperature,
            #max_tokens=max_tokens
        )
        
        return vector_store, llm, embeddings
        
    except Exception as e:
        st.error(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None, None, None

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
def get_prompt_template():
    template = """
    ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ì»¨í…ìŠ¤íŠ¸ì— ë‹µì´ ì—†ìœ¼ë©´ "ì œê³µëœ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.

    ì»¨í…ìŠ¤íŠ¸:
    {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€ì„ ì‘ì„±í•  ë•Œ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¥´ì„¸ìš”:
    1. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”
    2. ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
    3. í•„ìš”í•˜ë©´ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”
    4. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”

    ë‹µë³€:
    """
    return PromptTemplate(template=template, input_variables=["context", "question"])




# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    # í•„ìˆ˜ ì„¤ì • í™•ì¸
    if not all([llm_endpoint, llm_api_key, search_endpoint, search_key, index_name]):
        st.warning("âš ï¸ ëª¨ë“  í•„ìˆ˜ ì„¤ì •ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        return
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    vector_store, llm, embeddings = initialize_rag_system(llm_api_key,llm_endpoint,llm_api_version,llm_deployment,embedding_deployment,search_endpoint,search_key,
                          index_name,embedding_endpoint,embedding_api_key)
    if vector_store is None or llm is None:
        st.error("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # embeddings = AzureOpenAIEmbeddings(
    #     azure_deployment=azure_embedding_deployment,  # ì‹¤ì œ ë°°í¬ ì´ë¦„
    #     #openai_api_version="2024-02-01",
    #     azure_endpoint=embedding_endpoint,
    #     api_key=embedding_api_key 
    # )

    # vector_store = AzureSearch(
    #     azure_search_endpoint=search_endpoint,
    #     azure_search_key=search_key,
    #     index_name="langchain-vector-demo",  # ê¸°ì¡´ ì¸ë±ìŠ¤ ì´ë¦„
    #     embedding_function=embeddings,
    # )

    # llm = AzureOpenAI(
    # azure_endpoint=llm_endpoint,
    # api_key=llm_api_key,
    # api_version=llm_api_version,
    # model = llm_deployment
    # )
    # llm = AzureOpenAI(
    # azure_deployment=llm_deployment,
    # openai_api_version=llm_api_version,
    # azure_endpoint=llm_endpoint,
    # api_key=llm_api_key,
    #temperature=temperature,
    #max_tokens=max_tokens
    # )
   # =    llm_deployment

    # ì—°ê²° í…ŒìŠ¤íŠ¸
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("ğŸ” ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° í…ŒìŠ¤íŠ¸"):
            try:
                # ê°„ë‹¨í•œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                result = search_client.search("*", include_total_count=True, top=0)
                document_count = result.get_count()

                st.success(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° ì„±ê³µ! (ì¸ë±ìŠ¤: {index_name})")
                st.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {document_count}")
            except Exception as e:
                st.error(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    
    with col2:
        if st.button("ğŸ¤– LLM ì—°ê²° í…ŒìŠ¤íŠ¸"):
            try:
                test_response = llm.invoke("ì•ˆë…•í•˜ì„¸ìš”")
                #llm.completions.create(prompt = "ì•ˆë…•í•˜ì„¸ìš”")
                st.success("âœ… LLM ì—°ê²° ì„±ê³µ!")
                st.info(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response[:100]}...")
            except Exception as e:
                st.error(f"âŒ LLM ì—°ê²° ì‹¤íŒ¨: {str(e)}")
    
    st.divider()
    
    # ì§ˆì˜ì‘ë‹µ ì„¹ì…˜
    st.header("ğŸ’¬ ì§ˆì˜ì‘ë‹µ")
    
    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_input("ğŸ’­ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: íšŒì‚¬ì˜ ì£¼ìš” ì œí’ˆì€ ë¬´ì—‡ì¸ê°€ìš”?")
    
    col1, col2 = st.columns([1, 4])
    
    with col1:
        search_button = st.button("ğŸ” ê²€ìƒ‰", type="primary")
    
    with col2:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"):
            if 'chat_history' in st.session_state:
                del st.session_state['chat_history']
            st.rerun()
    
    # ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±
    if search_button and question:
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            try:
                # ê²€ìƒ‰ ì„¤ì •
                search_kwargs = {"k": k}
                if search_type == "similarity_score_threshold":
                    search_kwargs["score_threshold"] = score_threshold
                
                # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                if search_type == "mmr":
                    retrieved_docs = vector_store.max_marginal_relevance_search(
                        question, k=k
                    )
                elif search_type == "similarity_score_threshold":
                    retrieved_docs = vector_store.similarity_search_with_relevance_scores(
                        question, k=k
                    )
                    # ì„ê³„ê°’ í•„í„°ë§
                    retrieved_docs = [doc for doc, score in retrieved_docs if score >= score_threshold]
                else:
                    retrieved_docs = vector_store.similarity_search(question, k=k)
                
                # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
                st.subheader("ğŸ“š ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ")
                
                if retrieved_docs:
                    for i, doc in enumerate(retrieved_docs[:3]):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                        with st.expander(f"ë¬¸ì„œ {i+1} (ì¶œì²˜: {doc.metadata.get('source', 'Unknown')})"):
                            st.write(doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content)
                            st.json(doc.metadata)
                    
                    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # í”„ë¡¬í”„íŠ¸ ìƒì„±
                    prompt_template = get_prompt_template()
                    prompt = prompt_template.format(context=context, question=question)
                    
                    # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
                    st.subheader("ğŸ¤– AI ë‹µë³€")
                    
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        try:
                            response = llm.invoke(prompt)
                            st.write(response)
                            
                            # ëŒ€í™” ê¸°ë¡ ì €ì¥
                            if 'chat_history' not in st.session_state:
                                st.session_state['chat_history'] = []
                            
                            st.session_state['chat_history'].append({
                                "question": question,
                                "answer": response,
                                "retrieved_docs": len(retrieved_docs),
                                "timestamp": st.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            })
                            
                        except Exception as e:
                            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                
                else:
                    st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ì„¤ì •ì„ ì¡°ì •í•´ë³´ì„¸ìš”.")
                    
            except Exception as e:
                st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ
    if 'chat_history' in st.session_state and st.session_state['chat_history']:
        st.divider()
        st.header("ğŸ“‹ ëŒ€í™” ê¸°ë¡")
        
        for i, chat in enumerate(reversed(st.session_state['chat_history'])):
            with st.expander(f"ğŸ’­ {chat['question'][:50]}... ({chat['timestamp']})"):
                st.write("**ì§ˆë¬¸:**", chat['question'])
                st.write("**ë‹µë³€:**", chat['answer'])
                st.write(f"**ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:** {chat['retrieved_docs']}")

if __name__ == "__main__":
    main()